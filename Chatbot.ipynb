{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0590bd6-3d21-4cab-b51a-512a0733b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官方文档教程\n",
    "# https://python.langchain.com/docs/tutorials/chatbot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876db97e-aedb-419c-a4eb-6d8ef2fcfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建聊天机器人\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ead7f4-9651-43df-8e14-7e625b7be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 概述\n",
    "# 我们将通过一个示例，展示如何设计和实现一个基于 LLM 的聊天机器人。该聊天机器人将能够与聊天模型进行对话，并记住之前的交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79bcaa-c34f-4865-8e1f-3f562d258196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置\n",
    "# Jupyter Notebook\n",
    "# 本指南（以及文档中的大多数其他指南）均使用 Jupyter Notebook ，并假设读者也使用 Jupyter Notebook。Jupyter Notebook 非常适合学习如何使用 LLM 系统，因为经常会出现问题（例如意外输出、API 故障等），而在交互式环境中阅读指南是更好地理解这些系统的绝佳方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92572ab5-fefa-4491-9437-2fc4ada77cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装\n",
    "# 本教程需要 langchain-core 和 langgraph 。本指南需要 langgraph >= 0.2.28 。\n",
    "# conda install langchain-core langgraph>0.2.27 -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48944ea5-165d-4460-8e8c-8ae5d1adcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith\n",
    "# 使用 LangChain 构建的许多应用程序都包含多个步骤，需要多次调用 LLM 函数。随着这些应用程序变得越来越复杂，能够检查链或代理内部究竟发生了什么变得至关重要。最好的方法是使用 LangSmith 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a3c25-a5ff-4443-ac9d-2c2a997c26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过上面的链接注册后，请确保设置环境变量以开始记录跟踪：\n",
    "\n",
    "# export LANGSMITH_TRACING=\"true\"\n",
    "# export LANGSMITH_API_KEY=\"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81854275-3b92-47bc-8a77-fddba8d41b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 或者，如果在笔记本中，您可以使用以下方式设置它们：\n",
    "\n",
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d163636-0532-45bd-ba2a-50b39119be65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0b37a-0653-4b42-927c-803d7ffe7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速入门\n",
    "# 首先，让我们学习如何单独使用语言模型。LangChain 支持多种不同的语言模型，您可以互换使用 - 请在下方选择您想要使用的模型！\n",
    "# 选择聊天模型 ：\n",
    "# Google Gemini ▾\n",
    "# pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb72424-084c-40cd-ba7c-b5b64fc49661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80dd3e4d-ff08-4ebb-b1b0-0569ee3fb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先让我们直接使用模型。ChatModel 是 ChatModel “Runnables”的实例，这意味着它们暴露了一个用于交互的标准接口。为了简单地调用模型，我们可以将消息列表传递给 .invoke 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f00ed36-14e1-471b-8664-9dbb0f3f7772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Miku! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--04d948fe-7cb2-4500-b794-a409348980b3-0', usage_metadata={'input_tokens': 7, 'output_tokens': 20, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Miku\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cdf12b-11e0-4a16-ac36-dc476b55e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该模型本身没有任何状态概念。例如，如果你问一个后续问题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db845ed2-5ef9-494e-878a-63389b6c2596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I have no way of knowing your name. You haven't told me!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d4e62309-c9bb-4c04-aa3f-68fcfa5a4091-0', usage_metadata={'input_tokens': 6, 'output_tokens': 23, 'total_tokens': 29, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655bc5a-8134-45df-a3c3-6f2a7e0f368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们看一下 LangSmith 跟踪示例\n",
    "# 我们发现它没有将之前的对话内容纳入上下文，也无法回答问题。这给聊天机器人带来了糟糕的体验！\n",
    "# 为了解决这个问题，我们需要将整个对话历史记录传入模型。让我们看看这样做会发生什么：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa3d21a-a747-46fb-89fe-a85918d8cb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Miku. You just told me! 😊', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a079d517-e392-43a2-9bff-731199c61166-0', usage_metadata={'input_tokens': 24, 'output_tokens': 13, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Miku\"),\n",
    "        AIMessage(content=\"Hello Miku! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcabe0-2e20-4a9e-b39f-d9d8d8964578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们可以看到我们得到了良好的响应！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bd9b6-0c98-48d6-9522-66706b3998d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是聊天机器人对话交互能力的基本理念。那么，我们如何才能最好地实现这一点呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1dabad8-6ac1-4888-8166-fe6a32d73a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 消息持久化\n",
    "# LangGraph 实现了内置持久层，使其成为支持多轮对话的聊天应用程序的理想选择。\n",
    "# 将我们的聊天模型包装在一个最小的 LangGraph 应用程序中，使我们能够自动保存消息历史记录，从而简化多轮应用程序的开发。\n",
    "# LangGraph 自带一个简单的内存检查点，我们将在下面使用它。更多详细信息，包括如何使用不同的持久化后端（例如 SQLite 或 Postgres），请参阅其文档 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cafa242-856f-4d6b-aa4c-17593672bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1804056d-0bee-47e2-916a-ce5cfd1a175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在我们需要创建一个 config ，每次都传递给可运行对象。此配置包含一些不直接包含在输入中但仍然有用的信息。在本例中，我们希望包含一个 thread_id 。它应该如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c668e75a-0a8d-49a9-9b24-567346a0ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5277a5-2c95-41bc-9e75-4a340484bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这使我们能够使用单个应用程序支持多个对话线程，当您的应用程序有多个用户时，这是一个常见的要求。\n",
    "# 然后我们可以调用该应用程序：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09df3338-b9aa-4e54-b5e0-e51724251fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Miku! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Miku.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d966e69c-071e-486a-8b35-1dbf91c4900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Miku. You just told me! 😊\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26244c8b-dbb8-4e75-b87f-8b7221c718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 太棒了！我们的聊天机器人现在记住了我们的一些事情。如果我们修改配置，引用不同的 thread_id ，就能看到它重新开始对话了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d24c0b-70f0-4965-89e0-1d8fa7787797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As a large language model, I don't know your name. I have no memory of past conversations and don't have access to personal information. You haven't told me your name.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e2fb0df-42c8-45f4-8c4c-716613a7a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 但是，我们总是可以回到原始对话（因为我们将其保存在数据库中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565a18ea-9a94-438b-a61e-28f8b326a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You told me your name is Miku. Is that still correct?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77160585-45c0-4c71-bae6-a26b22b15bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这就是我们如何支持聊天机器人与许多用户进行对话！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de516904-c306-4406-b69d-9ff2d4ca496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip  提示\n",
    "# 对于异步支持，将 call_model 节点更新为异步函数，并在调用应用程序时使用 .ainvoke ：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feab2ce6-81c6-423d-b248-0e566e8d1b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As a large language model, I have no memory of past conversations. Therefore, I don't know your name. You haven't told me!\n"
     ]
    }
   ],
   "source": [
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d0698-d18d-48e5-9703-c7770e6af583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前，我们所做的只是在模型周围添加了一个简单的持久层。我们可以通过添加提示模板来使聊天机器人变得更加复杂和个性化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ced6-76d9-4b7b-bd7c-96c5e297460f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a561792c-ca6b-4372-8d43-43eb6716a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates(提示模板)\n",
    "# 提示模板有助于将原始用户信息转换为 LLM 可以处理的格式。\n",
    "# 在本例中，原始用户输入只是一条消息，我们会将其传递给 LLM。\n",
    "# 现在让我们让它更复杂一些。\n",
    "# 首先，让我们添加一条包含一些自定义指令的系统消息（但仍然接受消息作为输入）。\n",
    "# 接下来，除了消息之外，我们还将添加更多输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be462e8-39a3-4c8d-a8cd-88e1fb6f185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了添加系统消息，我们将创建一个 ChatPromptTemplate 。我们将利用 MessagesPlaceholder 来传递所有消息。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc02ea52-a9af-4a2b-a515-42fdc29c8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13aae62e-025d-4103-8ff2-9bd1be96650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们现在可以更新我们的应用程序以包含此模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cdd2a64-134f-4499-a180-64eac032fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419b78f-e858-4867-b189-1484a4b6b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们以同样的方式调用应用程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e961e444-a995-443a-8128-c0b797255599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, LuoTianYi! A pleasure to meet ye on the digital seas! What brings ye to these waters? I be ready to lend a hand, or at least spin a yarn or two.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm LuoTianYi.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd6ad1a8-4abe-4664-93ab-2e52a2736d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Shiver me timbers, ye be testin' me memory, aye? Yer name be LuoTianYi, as ye told me just a moment ago! Don't be makin' this ol' salt doubt his senses!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394fa94-9bd5-44ea-a685-dfc2c6b41281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 太棒了！现在让我们把提示符变得更复杂一些。假设提示符模板现在看起来像这样：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61042180-addc-4380-9261-bc8a8a581bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "544c7e4c-0af8-4e2d-b6a3-dcf5a0bab9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请注意，我们在提示框中添加了新的 language 输入。我们的应用现在有两个参数——输入 messages 和 language 。我们应该更新应用的状态以反映这一点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c28589cd-90a7-439d-b4f2-c57a0a7315e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47ada81f-439b-4a1a-b72d-7f714211caf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，洛天依！很高兴认识你。有什么我可以帮你的吗？\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm LuoTianYi.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc05cb-72a7-4415-866b-1f2c02c3728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请注意，整个状态是持久的，因此如果不需要更改，我们可以省略 language 等参数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b9caa71-0d1f-4549-a944-e7bb2fcf518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的名字是洛天依。\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11fd5a-0c02-4f82-9207-3fb2b2f9866e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676bfcc-cf93-4b8f-9f3a-7d241b3add51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing Conversation History（管理对话历史记录）\n",
    "# 构建聊天机器人时，需要理解的一个重要概念是如何管理对话历史记录。如果不加以管理，消息列表将无限增长，并可能溢出 LLM 的上下文窗口。因此，添加一个限制传入消息大小的步骤非常重要。\n",
    "# 重要的是，您需要在提示模板之前但在从消息历史记录中加载以前的消息之后执行此操作。\n",
    "# 我们可以在提示前添加一个简单的步骤来适当修改 messages 键，然后将该新链包装在消息历史记录类中。\n",
    "# LangChain 自带一些内置助手，用于管理消息列表 。在本例中，我们将使用 trim_messages 助手来减少发送给模型的消息数量。修剪器允许我们指定要保留的标记数量，以及其他参数，例如是否要始终保留系统消息以及是否允许部分消息：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "824cabfa-ec15-4ce7-8504-a14f4f3de6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7ed03-1457-4424-86ae-491be3d0f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了在我们的链中使用它，我们只需要在将 messages 输入传递给提示之前运行修剪器。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0a4d7ce-890f-40e1-8405-cca0d11f264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a26b47dc-67d9-4d4e-ba6b-660851950434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在，如果我们尝试询问模型我们的名字，它将不会知道，因为我们修剪了聊天记录的那一部分：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1bb8944-5388-4202-97fc-1ee1923b5e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "作为一个AI，我无法知道你的名字。你没有告诉我你的名字。\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb325c6-619a-42e9-8a8d-c83828522c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 但如果我们询问最近几条消息中的信息，它会记住："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "992940e4-9b74-444e-80e3-cae4162364c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你问的数学问题是：2 + 2 等于多少？\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324d4f6-b0a8-4c46-9a9f-1fc2e43887eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果您看一下 LangSmith，您就可以清楚地看到 LangSmith 跟踪中幕后发生的事情。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b09a3-edd4-4977-91b6-b4d28c8df04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming(流式传输)\n",
    "# 现在，我们已经有了一个可以运行的聊天机器人。然而，对于聊天机器人应用程序来说，一个非常重要的用户体验考虑因素是流式传输。LLM 有时可能需要一段时间才能响应，因此为了提升用户体验，大多数应用程序都会在每个令牌生成时将其流式传输回来。这让用户能够看到进度。\n",
    "# 默认情况下，我们的 LangGraph 应用程序中的 .stream 会流式传输应用程序步骤——在本例中，是模型响应的单个步骤。设置 stream_mode=\"messages\" 允许我们改为流式传输输出令牌：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bce4d5d8-20ca-4eff-93ab-5865c5f7f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好|奥|特曼！\n",
      "\n",
      "这里有个笑话给你：\n",
      "\n",
      "为什么奥特曼总是|赢？\n",
      "\n",
      "因为他总是“奥特”一把！ (因为他总是“all| out” 一把， “奥特” 和 “all out” 发音相似)\n",
      "\n",
      "希望你喜欢！\n",
      "|"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Ultraman, please tell me a joke.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80c695-c532-41f3-8dd5-fd208acfb580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74bdef5-6a59-41a4-bf42-76b3f19d6ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac673e1-1211-491d-8822-690d73f1b117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
