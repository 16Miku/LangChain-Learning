import os
import asyncio
from typing import Optional, Dict, Any
from langchain_core.tools import tool
from langchain_qa_backend import (
    ingest_url,
    ingest_file,
    get_retrieval_chain
)
import numpy as np

def clean_metadata(metadata: dict) -> dict:
    """Recursively convert numpy types to python types for JSON serialization"""
    cleaned = {}
    for key, value in metadata.items():
        if isinstance(value, np.float32):
            cleaned[key] = float(value)
        elif isinstance(value, dict):
            cleaned[key] = clean_metadata(value)
        else:
            cleaned[key] = value
    return cleaned

@tool
async def ingest_knowledge(source: str, type: str):
    """
    ç»Ÿä¸€çš„çŸ¥è¯†æ‘„å–å·¥å…·ã€‚å°†ç½‘é¡µURLæˆ–æœ¬åœ°æ–‡ä»¶å†…å®¹æ‘„å–åˆ°ç»Ÿä¸€çš„å‘é‡çŸ¥è¯†åº“ä¸­ã€‚
    
    Args:
        source (str): èµ„æºè·¯å¾„ã€‚å¦‚æœæ˜¯URLåˆ™ä¸ºhttpé“¾æ¥ï¼Œå¦‚æœæ˜¯æ–‡ä»¶åˆ™ä¸ºæ–‡ä»¶åï¼ˆåç«¯å·²ä¿å­˜åˆ°ä¸´æ—¶åŒºï¼‰ã€‚
        type (str): èµ„æºç±»å‹ã€‚å¯é€‰å€¼: 'url', 'file'ã€‚
    """
    print(f"\nğŸ“š [Knowledge] æ­£åœ¨æ‘„å–çŸ¥è¯†åº“: {source} (Type: {type})...")
    
    success = False
    if type == 'url':
        success = await ingest_url(source)
    elif type == 'file':
        # For file ingestion, the backend API should have already saved the file 
        # and passed the path. However, 'source' here is likely just the filename 
        # if called by the LLM based on user context.
        # We assume the file is in a temporary holding area known to the backend.
        # For simplicity in this tool, we might need the full path.
        # Let's assume 'source' passed by Agent is the filename user sees.
        # We need to look it up in a temp dir.
        # On Vercel, only /tmp is writable
        temp_dir = "/tmp/temp_uploads" 
        filepath = os.path.join(temp_dir, source)
        
        if os.path.exists(filepath):
            success = await ingest_file(filepath, source)
        else:
            return f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {source}ã€‚è¯·ç¡®è®¤æ–‡ä»¶å·²ä¸Šä¼ ã€‚"
    else:
        return f"âŒ é”™è¯¯: ä¸æ”¯æŒçš„ç±»å‹ {type}"
    
    if success:
        print(f"âœ… [Knowledge] çŸ¥è¯†åº“æ‘„å–å®Œæˆ: {source}")
        return f"æˆåŠŸå­¦ä¹ äº† {type} å†…å®¹: {source}"
    else:
        return f"âŒ é”™è¯¯: æ— æ³•å¤„ç† {source}"

@tool
async def query_knowledge_base(query: str, source_filter: Optional[str] = None):
    """
    ç»Ÿä¸€çš„çŸ¥è¯†åº“æŸ¥è¯¢å·¥å…·ã€‚
    
    Args:
        query (str): ç”¨æˆ·çš„å…·ä½“é—®é¢˜ã€‚
        source_filter (Optional[str]): å¯é€‰ã€‚å¦‚æœæŒ‡å®šï¼Œä»…ä»è¯¥ç‰¹å®šçš„ URL æˆ–æ–‡ä»¶åä¸­æ£€ç´¢ç­”æ¡ˆã€‚
                                     å¦‚æœä¸æŒ‡å®šï¼Œåˆ™ä»æ•´ä¸ªçŸ¥è¯†åº“ä¸­æ£€ç´¢ã€‚
    """
    filter_msg = f" (Filter: {source_filter})" if source_filter else " (Global Search)"
    print(f"\nğŸ¤” [RAG] æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“: {query}{filter_msg} ...")
    
    try:
        # Create chain on the fly (lightweight)
        chain = get_retrieval_chain(source_filter)
        
        response = await chain.ainvoke({
            "input": query,
            "chat_history": [] 
        })
        
        answer = response["answer"]
        source_documents = response.get("context", [])
        
        # Format sources
        sources_text = ""
        for i, doc in enumerate(source_documents[:3]):
            cleaned_meta = clean_metadata(doc.metadata)
            src = cleaned_meta.get("source", "Unknown")
            sources_text += f"\n- [{i+1}] {src}: {doc.page_content[:100]}..."

        final_output = f"{answer}\n\nå‚è€ƒæ¥æº:{sources_text}"
        print(f"âœ… [RAG] æŸ¥è¯¢å®Œæˆã€‚")
        return final_output

    except Exception as e:
        print(f"âŒ [RAG] æŸ¥è¯¢å‡ºé”™: {e}")
        return f"æŸ¥è¯¢çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}"
