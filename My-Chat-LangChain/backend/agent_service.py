import os
import asyncio
import aiosqlite
from typing import Dict, Any, List
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient


import logging
logging.getLogger("mcp").setLevel(logging.ERROR)
logging.getLogger("root").setLevel(logging.ERROR)
# ç»†å¾®ä¼˜åŒ–
# ä¹‹å‰æ—¥å¿—ä¸­æœ‰ä¸ªå° Warningï¼š
# WARNING:root:Failed to validate notification: 11 validation errors...
# è¿™æ˜¯ MCP åè®®çš„åº•å±‚æ—¥å¿—ï¼Œä¸å½±å“ä¸šåŠ¡ï¼Œä½†çœ‹ç€å¿ƒçƒ¦ã€‚å¯ä»¥é€šè¿‡è°ƒæ•´ logging çº§åˆ«æ¥å±è”½ï¼š



# Import custom tools
from tools.search_tools import generate_search_queries, execute_searches_and_get_urls
from tools.rag_tools import ingest_knowledge, query_knowledge_base
from tools.structure_tools import format_paper_analysis, format_linkedin_profile

load_dotenv()

# Global variables
_agent_executor = None
_mcp_client = None
_mcp_tools = []
_sqlite_conn = None

# --- Persistence Config ---
# On Vercel, only /tmp is writable
DB_PATH = "/tmp/data/state.db"
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½çš„ AI ç ”ç©¶åŠ©ç† (Stream-Agent v6.0)ã€‚
ä½ å¯ä»¥å¤„ç†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†æžå­¦æœ¯è®ºæ–‡ã€æŸ¥è¯¢ä¸ªäººèµ„æ–™ã€æ‰§è¡Œå¤æ‚çš„ç½‘ç»œæœç´¢ï¼Œä»¥åŠæ·±å…¥å­¦ä¹ å’ŒæŸ¥è¯¢ç‰¹å®šçš„ç½‘é¡µ/æ–‡ä»¶çŸ¥è¯†åº“ã€‚

**ä½ çš„èƒ½åŠ› (å·¥å…·ç®±):**
1.  **RAG çŸ¥è¯†åº“å·¥å…· (ç»Ÿä¸€å…¥å£)**:
    *   `ingest_knowledge(source, type)`: å­¦ä¹ æ–°çŸ¥è¯†ã€‚`source`å¯ä»¥æ˜¯URLæˆ–ä¸Šä¼ çš„æ–‡ä»¶åã€‚
    *   `query_knowledge_base(query, source_filter)`: æŸ¥è¯¢çŸ¥è¯†åº“ã€‚å¯ä»¥æŒ‡å®š `source_filter` æ¥åªæŸ¥ç‰¹å®šæ–‡æ¡£ã€‚
2.  **æœç´¢ä¸Žåˆ†æžå·¥å…·**:
    *   `generate_search_queries`: åˆ†æžç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆæœç´¢ç­–ç•¥ã€‚
    *   `execute_searches_and_get_urls`: æ‰§è¡Œæœç´¢ã€‚
    *   ä»¥åŠæ¥è‡ª MCP (å¦‚ BrightData, PaperSearch) çš„å…¶ä»–å¼ºå¤§å·¥å…·ï¼ˆå¦‚æžœå·²é…ç½®ï¼‰ã€‚
3.  **ç»“æž„åŒ–æŠ¥å‘Šå·¥å…·**:
    *   `format_paper_analysis`: ç”Ÿæˆè®ºæ–‡åˆ†æžæŠ¥å‘Šã€‚
    *   `format_linkedin_profile`: ç”Ÿæˆé¢†è‹±ä¸ªäººä¸»é¡µæŠ¥å‘Šã€‚

**ä½ çš„è¡ŒåŠ¨æŒ‡å— (ReAct æ€è€ƒæ¨¡å¼):**
1.  **åˆ†æžä¸Žè§„åˆ’**: ä»”ç»†é˜…è¯»ç”¨æˆ·çš„è¯·æ±‚ã€‚
    *   ç”¨æˆ·ä¸Šä¼ äº†æ–‡ä»¶? -> è‡ªåŠ¨è°ƒç”¨ `ingest_knowledge(filename, 'file')`ã€‚
    *   ç”¨æˆ·å‘äº†é“¾æŽ¥? -> è‡ªåŠ¨è°ƒç”¨ `ingest_knowledge(url, 'url')`ã€‚
    *   ç”¨æˆ·é—®å…³äºŽåˆšæ‰æ–‡ä»¶çš„é—®é¢˜? -> `query_knowledge_base(query, filename)`ã€‚
    *   ç”¨æˆ·éœ€è¦åšç ”ç©¶? -> `generate_search_queries` -> `execute_searches`ã€‚
2.  **ä¿¡æ¯æ”¶é›†**: çµæ´»ç»„åˆä½¿ç”¨ä½ çš„å·¥å…·ã€‚
3.  **ç”Ÿæˆå›žç­”**: ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

**æ³¨æ„äº‹é¡¹**:
*   å¦‚æžœç”¨æˆ·æåˆ°â€œåˆšä¸Šä¼ çš„æ–‡ä»¶â€ï¼Œè¯·æ£€æŸ¥ä¸Šä¸‹æ–‡ä¸­çš„æ–‡ä»¶åã€‚
*   å¯¹äºŽ RAG ä»»åŠ¡ï¼Œä¼˜å…ˆå°è¯•ç²¾ç¡®è¿‡æ»¤æŸ¥è¯¢ (`source_filter`)ï¼Œå¦‚æžœæ— ç»“æžœå†å°è¯•å…¨å±€æŸ¥è¯¢ã€‚
"""

async def initialize_agent(api_keys: Dict[str, str] = None):
    """
    Initialize the LangGraph agent with MCP tools, custom tools, and SQLite persistence.
    """
    global _agent_executor, _mcp_client, _mcp_tools, _sqlite_conn

    print("ðŸš€ [Agent Service] Initializing Agent with Persistence...")
    
    # 1. Configure MCP Client (Same as before)
    mcp_servers = {}
    bd_key = api_keys.get("BRIGHT_DATA_API_KEY") if api_keys else os.environ.get("BRIGHT_DATA_API_KEY")
    if bd_key:
        mcp_servers["bright_data"] = {
            "url": f"https://mcp.brightdata.com/mcp?token={bd_key}&pro=1",
            "transport": "streamable_http",
        }
    ps_key = api_keys.get("PAPER_SEARCH_API_KEY") if api_keys else os.environ.get("PAPER_SEARCH_API_KEY")
    if ps_key:
        mcp_servers["paper_search"] = {
            "url": f"https://server.smithery.ai/@adamamer20/paper-search-mcp-openai/mcp?api_key={ps_key}",
            "transport": "streamable_http",
        }

    custom_tools = [
        generate_search_queries, 
        execute_searches_and_get_urls,
        ingest_knowledge, 
        query_knowledge_base,
        format_paper_analysis,
        format_linkedin_profile
    ]

    if mcp_servers:
        try:
            _mcp_client = MultiServerMCPClient(mcp_servers)
            try:
                _mcp_tools = await _mcp_client.get_tools()
                print(f"âœ… [Agent Service] Loaded {len(_mcp_tools)} MCP tools.")
            except Exception as e:
                print(f"âš ï¸ [Agent Service] Failed to load MCP tools: {e}")
                _mcp_tools = []
        except Exception as e:
            print(f"âš ï¸ [Agent Service] Failed to connect to MCP servers: {e}")
            _mcp_tools = []
    else:
        _mcp_tools = []

    all_tools = _mcp_tools + custom_tools

    # 2. Configure LLM
    if "GOOGLE_API_KEY" not in os.environ:
        raise ValueError("GOOGLE_API_KEY is missing!")
        
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0
    )

    # 3. Create LangGraph Agent with AsyncSqliteSaver
    if _sqlite_conn is None:
        _sqlite_conn = await aiosqlite.connect(DB_PATH)
        
    checkpointer = AsyncSqliteSaver(_sqlite_conn)
    
    _agent_executor = create_react_agent(
        model=llm,
        tools=all_tools,
        checkpointer=checkpointer
    )
    
    print("âœ… [Agent Service] Persistent Agent initialized successfully.")
    return _agent_executor

async def get_agent_executor():
    global _agent_executor
    if _agent_executor is None:
        await initialize_agent()
    return _agent_executor

async def chat_with_agent(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Main entry point for chatting (Synchronous return for now, will be upgraded to stream).
    """
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    config = {"configurable": {"thread_id": thread_id}}
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]
    
    final_state = await agent.ainvoke(
        {"messages": messages},
        config=config
    )
    
    return final_state["messages"][-1].content

async def chat_with_agent_stream(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Generator function for streaming agent responses and thoughts.
    """
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    config = {"configurable": {"thread_id": thread_id}}
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]

    async for event in agent.astream_events({"messages": messages}, config=config, version="v1"):
        kind = event["event"]
        
        # Yield different event types for the frontend to consume
        if kind == "on_chat_model_stream":
            content = event["data"]["chunk"].content
            if content:
                yield f"data: {content}\n\n"
        
        elif kind == "on_tool_start":
            tool_name = event["name"]
            yield f"event: tool_start\ndata: {tool_name}\n\n"
            
        elif kind == "on_tool_end":
            tool_name = event["name"]
            output = str(event["data"].get("output"))
            # Truncate long outputs for display
            safe_output = (output[:200] + '...') if len(output) > 200 else output
            # JSON encoded to avoid newline issues in SSE
            import json
