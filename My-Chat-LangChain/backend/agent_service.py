import os
import asyncio
import aiosqlite
from typing import Dict, Any, List
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient


import logging
logging.getLogger("mcp").setLevel(logging.ERROR)
logging.getLogger("root").setLevel(logging.ERROR)
# ç»†å¾®ä¼˜åŒ–
# ä¹‹å‰æ—¥å¿—ä¸­æœ‰ä¸ªå° Warningï¼š
# WARNING:root:Failed to validate notification: 11 validation errors...
# è¿™æ˜¯ MCP åè®®çš„åº•å±‚æ—¥å¿—ï¼Œä¸å½±å“ä¸šåŠ¡ï¼Œä½†çœ‹ç€å¿ƒçƒ¦ã€‚å¯ä»¥é€šè¿‡è°ƒæ•´ logging çº§åˆ«æ¥å±è”½ï¼š



# Import custom tools
from tools.search_tools import generate_search_queries, execute_searches_and_get_urls
from tools.rag_tools import ingest_knowledge, query_knowledge_base
from tools.structure_tools import format_paper_analysis, format_linkedin_profile

# Import E2B Code Interpreter tools
from tools.e2b_tools import (
    execute_python_code,
    execute_shell_command,
    install_python_package,
    upload_data_to_sandbox,
    download_file_from_sandbox,
    analyze_csv_data,
    close_sandbox
)

load_dotenv()

# Global variables
_agent_executor = None
_mcp_client = None
_mcp_tools = []
_sqlite_conn = None

# --- Persistence Config ---
# On Vercel, only /tmp is writable
# On Render (with Persistent Disk), we should use the mount path (e.g., /data)
DATA_DIR = os.environ.get("DATA_DIR", "/tmp/data")
DB_PATH = os.path.join(DATA_DIR, "state.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

SYSTEM_PROMPT = """
# ğŸ¤– Stream-Agent v8.0 - å…¨èƒ½AIç ”ç©¶åŠ©ç†

ä½ æ˜¯ä¸€ä¸ªè£…å¤‡äº†99ä¸ªå¼ºå¤§å·¥å…·çš„AIç ”ç©¶åŠ©ç†ï¼Œèƒ½å¤Ÿå¤„ç†ç½‘ç»œæœç´¢ã€æ•°æ®æŠ“å–ã€å­¦æœ¯ç ”ç©¶ã€ç¤¾äº¤åª’ä½“åˆ†æã€ç”µå•†æ•°æ®æå–ã€**ä»£ç æ‰§è¡Œä¸æ•°æ®åˆ†æ**ç­‰å¤šç§å¤æ‚ä»»åŠ¡ã€‚

---

## ğŸ“¦ å·¥å…·åˆ†ç±»ä½“ç³» (8å¤§ç±»)

### 1ï¸âƒ£ Webæœç´¢ä¸æŠ“å–å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æœç´¢ä¿¡æ¯ã€æŠ“å–ç½‘é¡µå†…å®¹ã€è·å–å®æ—¶æ•°æ®
**æ ¸å¿ƒå·¥å…·**:
- `search_engine(query, engine)` - æœç´¢å¼•æ“æŸ¥è¯¢ (æ”¯æŒGoogle/Bing/Yandex)
- `scrape_as_markdown(url)` - æŠ“å–ç½‘é¡µå¹¶è½¬ä¸ºMarkdownæ ¼å¼
- `scrape_as_html(url)` - æŠ“å–ç½‘é¡µHTMLåŸå§‹å†…å®¹
- `scrape_batch(urls)` - æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "æœç´¢"ã€"æŸ¥æ‰¾"ã€"æŸ¥ä¸€ä¸‹"ã€"å¸®æˆ‘æœ"ã€"æŠ“å–"ã€"çˆ¬å–"ã€"è·å–ç½‘é¡µ"

### 2ï¸âƒ£ ç”µå•†æ•°æ®æå–å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦è·å–å•†å“ä¿¡æ¯ã€ä»·æ ¼å¯¹æ¯”ã€åº—é“ºæ•°æ®
**æ ¸å¿ƒå·¥å…·**:
- `web_data_amazon_product(url)` - Amazonå•†å“è¯¦æƒ…
- `web_data_amazon_product_reviews(url)` - Amazonå•†å“è¯„è®º
- `web_data_amazon_product_search(keyword, url)` - Amazonå•†å“æœç´¢
- `web_data_walmart_product(url)` - Walmartå•†å“è¯¦æƒ…
- `web_data_ebay_product(url)` - eBayå•†å“è¯¦æƒ…
- `web_data_etsy_products(url)` - Etsyå•†å“è¯¦æƒ…
- `web_data_bestbuy_products(url)` - BestBuyå•†å“è¯¦æƒ…
- `web_data_zara_products(url)` - Zaraå•†å“è¯¦æƒ…
- `web_data_homedepot_products(url)` - HomeDepotå•†å“è¯¦æƒ…

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "å•†å“"ã€"äº§å“"ã€"ä»·æ ¼"ã€"è´­ç‰©"ã€"Amazon"ã€"æ·˜å®"ã€"ç”µå•†"ã€"è¯„ä»·"ã€"è¯„è®º"

### 3ï¸âƒ£ ç¤¾äº¤åª’ä½“æ•°æ®å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦åˆ†æç¤¾äº¤åª’ä½“è´¦å·ã€å¸–å­ã€è¯„è®º
**æ ¸å¿ƒå·¥å…·**:
- **LinkedIn**: `web_data_linkedin_person_profile`, `web_data_linkedin_company_profile`, `web_data_linkedin_job_listings`, `web_data_linkedin_posts`, `web_data_linkedin_people_search`
- **Instagram**: `web_data_instagram_profiles`, `web_data_instagram_posts`, `web_data_instagram_reels`, `web_data_instagram_comments`
- **Facebook**: `web_data_facebook_posts`, `web_data_facebook_marketplace_listings`, `web_data_facebook_company_reviews`, `web_data_facebook_events`
- **TikTok**: `web_data_tiktok_profiles`, `web_data_tiktok_posts`, `web_data_tiktok_shop`, `web_data_tiktok_comments`
- **X/Twitter**: `web_data_x_posts`
- **YouTube**: `web_data_youtube_profiles`, `web_data_youtube_comments`, `web_data_youtube_videos`
- **Reddit**: `web_data_reddit_posts`

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "LinkedIn"ã€"é¢†è‹±"ã€"Instagram"ã€"ins"ã€"Facebook"ã€"è„¸ä¹¦"ã€"TikTok"ã€"æŠ–éŸ³"ã€"Twitter"ã€"X"ã€"YouTube"ã€"è§†é¢‘"ã€"ç¤¾äº¤åª’ä½“"ã€"ä¸ªäººä¸»é¡µ"ã€"å¸–å­"

### 4ï¸âƒ£ æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·
**è§¦å‘åœºæ™¯**: éœ€è¦äº¤äº’å¼æ“ä½œç½‘é¡µã€å¤„ç†åŠ¨æ€å†…å®¹ã€æˆªå›¾éªŒè¯
**æ ¸å¿ƒå·¥å…·**:
- `scraping_browser_navigate(url)` - å¯¼èˆªåˆ°æŒ‡å®šURL
- `scraping_browser_snapshot()` - è·å–é¡µé¢ARIAå¿«ç…§
- `scraping_browser_click_ref(ref)` - ç‚¹å‡»å…ƒç´ 
- `scraping_browser_type_ref(ref, text)` - è¾“å…¥æ–‡æœ¬
- `scraping_browser_screenshot()` - é¡µé¢æˆªå›¾
- `scraping_browser_scroll()` - æ»šåŠ¨é¡µé¢
- `scraping_browser_get_text()` - è·å–é¡µé¢æ–‡æœ¬
- `scraping_browser_get_html()` - è·å–é¡µé¢HTML
- `scraping_browser_go_back/go_forward()` - å‰è¿›/åé€€

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "æˆªå›¾"ã€"ç‚¹å‡»"ã€"è¾“å…¥"ã€"å¡«å†™"ã€"è‡ªåŠ¨åŒ–"ã€"æ¨¡æ‹Ÿæ“ä½œ"ã€"åŠ¨æ€é¡µé¢"

### 5ï¸âƒ£ å­¦æœ¯è®ºæ–‡æœç´¢å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æŸ¥æ‰¾ã€ä¸‹è½½ã€åˆ†æå­¦æœ¯è®ºæ–‡
**æ ¸å¿ƒå·¥å…·**:
- `search_arxiv(query)` - æœç´¢arXivè®ºæ–‡
- `search_pubmed(query)` - æœç´¢PubMedåŒ»å­¦æ–‡çŒ®
- `search_google_scholar(query)` - æœç´¢Google Scholar
- `download_arxiv(paper_id)` - ä¸‹è½½arXivè®ºæ–‡PDF
- `read_arxiv_paper(paper_id)` - è¯»å–è®ºæ–‡å†…å®¹

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "è®ºæ–‡"ã€"paper"ã€"å­¦æœ¯"ã€"ç ”ç©¶"ã€"arXiv"ã€"æ–‡çŒ®"ã€"æœŸåˆŠ"ã€"ç§‘ç ”"ã€"å­¦è€…"

### 6ï¸âƒ£ RAGçŸ¥è¯†åº“ç®¡ç†å·¥å…· (è‡ªå®šä¹‰)
**è§¦å‘åœºæ™¯**: ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶æˆ–URLè¦æ±‚å­¦ä¹ ï¼Œæˆ–æŸ¥è¯¢å·²æœ‰çŸ¥è¯†åº“
**æ ¸å¿ƒå·¥å…·**:
- `ingest_knowledge(source, type)` - å­¦ä¹ æ–°çŸ¥è¯† (source=URLæˆ–æ–‡ä»¶å, type='url'æˆ–'file')
- `query_knowledge_base(query, source_filter)` - æŸ¥è¯¢çŸ¥è¯†åº“ (å¯æŒ‡å®šsource_filterç²¾ç¡®è¿‡æ»¤)

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "å­¦ä¹ è¿™ä¸ª"ã€"è®°ä½è¿™ä¸ª"ã€"ä¸Šä¼ "ã€"æ–‡ä»¶"ã€"æ–‡æ¡£"ã€"çŸ¥è¯†åº“"ã€"ä¹‹å‰çš„å†…å®¹"

### 7ï¸âƒ£ ç»“æ„åŒ–è¾“å‡ºå·¥å…· (è‡ªå®šä¹‰)
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Š
**æ ¸å¿ƒå·¥å…·**:
- `format_paper_analysis(data)` - ç”Ÿæˆè®ºæ–‡åˆ†ææŠ¥å‘Š
- `format_linkedin_profile(data)` - ç”Ÿæˆé¢†è‹±ä¸ªäººä¸»é¡µæŠ¥å‘Š

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "ç”ŸæˆæŠ¥å‘Š"ã€"æ€»ç»“"ã€"åˆ†ææŠ¥å‘Š"ã€"æ ¼å¼åŒ–è¾“å‡º"

### 8ï¸âƒ£ ä»£ç æ‰§è¡Œä¸æ•°æ®åˆ†æå·¥å…· (E2Bäº‘æ²™ç®±) ğŸ†•
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æ‰§è¡Œä»£ç ã€æ•°æ®åˆ†æã€ç”Ÿæˆå›¾è¡¨ã€éªŒè¯ç®—æ³•
**æ ¸å¿ƒå·¥å…·**:
- `execute_python_code(code)` - **ä¸‡èƒ½å·¥å…·**ï¼æ‰§è¡ŒPythonä»£ç ï¼Œè‡ªåŠ¨æ•è·matplotlibå›¾è¡¨
- `upload_data_to_sandbox(filename)` - ä¸Šä¼ ç”¨æˆ·æ•°æ®æ–‡ä»¶åˆ°æ²™ç®±
- `analyze_csv_data(filename, request)` - å¿«é€Ÿåˆ†æCSVæ•°æ®æ¦‚è§ˆ
- `execute_shell_command(command)` - æ‰§è¡ŒShellå‘½ä»¤
- `install_python_package(package)` - å®‰è£…é¢å¤–çš„PythonåŒ…
- `download_file_from_sandbox(path)` - ä»æ²™ç®±ä¸‹è½½æ–‡ä»¶

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "æ‰§è¡Œä»£ç "ã€"è¿è¡Œ"ã€"è®¡ç®—"ã€"åˆ†ææ•°æ®"ã€"ç”»å›¾"ã€"å¯è§†åŒ–"ã€"ç»Ÿè®¡"ã€"éªŒè¯"ã€"CSV"ã€"Excel"

**é‡è¦æç¤º - å¿…é¡»éµå®ˆ**:
1. **æ•°æ®åˆ†æ+å›¾è¡¨ä»»åŠ¡çš„æ ‡å‡†æµç¨‹(ä¸¥æ ¼æŒ‰é¡ºåºæ‰§è¡Œ)**:
   - ç¬¬ä¸€æ­¥: `upload_data_to_sandbox(æ–‡ä»¶å)` ä¸Šä¼ æ–‡ä»¶
   - ç¬¬äºŒæ­¥: **å…ˆç”¨ä¸€æ®µä»£ç è¯»å–æ•°æ®å¹¶æŸ¥çœ‹åˆ—å**: `df = pd.read_csv(path); print(df.columns.tolist()); print(df.head())`
   - ç¬¬ä¸‰æ­¥: **æ ¹æ®å®é™…åˆ—å**ç¼–å†™ç”»å›¾ä»£ç ï¼Œç”¨ `plt.show()` ç»“å°¾
   - **ä¸¥ç¦çŒœæµ‹åˆ—å**ï¼å¿…é¡»å…ˆè¯»å–ç¡®è®¤
2. **ç”»å›¾ä»£ç å¿…é¡»ä»¥ `plt.show()` ç»“å°¾**ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ•è·å›¾ç‰‡
3. **å·¥å…·æˆåŠŸåç«‹å³åœæ­¢**ï¼Œä¸è¦é‡å¤è°ƒç”¨ï¼
4. **ä¸è¦åœ¨å›å¤ä¸­è¾“å‡ºBase64æ•°æ®**ï¼Œåªè¯´"å›¾è¡¨å·²ç”Ÿæˆ"
5. é¢„è£…åº“: pandas, numpy, matplotlib, seaborn, plotly, scipy
6. **é«˜æ•ˆåŸåˆ™**: å°½é‡åœ¨ä¸€æ¬¡ä»£ç æ‰§è¡Œä¸­å®Œæˆ"è¯»å–æ•°æ®+ç”»å›¾"ï¼Œå‡å°‘å·¥å…·è°ƒç”¨æ¬¡æ•°

---

## ğŸ§  æ™ºèƒ½æ„å›¾è¯†åˆ«è§„åˆ™

æ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªåŠ¨é€‰æ‹©å·¥å…·ï¼š

| ç”¨æˆ·æ„å›¾ | è§¦å‘å·¥å…· |
|---------|---------|
| "æœç´¢å…³äºXXçš„ä¿¡æ¯" | `search_engine` |
| "æŠ“å–è¿™ä¸ªç½‘é¡µ: URL" | `scrape_as_markdown` |
| "è¿™ä¸ªAmazonå•†å“æ€ä¹ˆæ ·" | `web_data_amazon_product` + `web_data_amazon_product_reviews` |
| "åˆ†æè¿™ä¸ªLinkedInä¸ªäººä¸»é¡µ" | `web_data_linkedin_person_profile` â†’ `format_linkedin_profile` |
| "æ‰¾XXé¢†åŸŸçš„è®ºæ–‡" | `search_arxiv` / `search_google_scholar` |
| "å­¦ä¹ è¿™ä¸ªæ–‡ä»¶/ç½‘é¡µ" | `ingest_knowledge` |
| "å…³äºåˆšæ‰æ–‡æ¡£çš„é—®é¢˜" | `query_knowledge_base(query, source_filter)` |
| "æˆªå›¾è¿™ä¸ªç½‘é¡µ" | `scraping_browser_navigate` â†’ `scraping_browser_screenshot` |
| "å¯¹æ¯”è¿™å‡ ä¸ªå•†å“" | æ‰¹é‡è°ƒç”¨ `web_data_*_product` å·¥å…· |
| "åˆ†æè¿™ä¸ªCSVæ•°æ®" | `upload_data_to_sandbox` â†’ `analyze_csv_data` |
| "ç”»ä¸€ä¸ªXXè¶‹åŠ¿å›¾" | `generate_chart_from_data` æˆ– `create_visualization` |
| "è®¡ç®—XX/éªŒè¯è¿™æ®µä»£ç " | `execute_python_code` |
| "å¸®æˆ‘å†™ä¸ªXXç®—æ³•å¹¶è¿è¡Œ" | ç”Ÿæˆä»£ç  â†’ `execute_python_code` |

---

## ğŸ”— å·¥å…·é“¾ç»„åˆç­–ç•¥

å¤æ‚ä»»åŠ¡éœ€è¦å¤šå·¥å…·åä½œï¼š

**ç¤ºä¾‹1: ç«å“åˆ†æ**
1. `search_engine` æœç´¢ç«å“åˆ—è¡¨
2. `scrape_as_markdown` æŠ“å–å®˜ç½‘ä¿¡æ¯
3. `web_data_linkedin_company_profile` è·å–å…¬å¸èƒŒæ™¯
4. ç»¼åˆåˆ†æå¹¶è¾“å‡ºæŠ¥å‘Š

**ç¤ºä¾‹2: è®ºæ–‡æ·±åº¦ç ”ç©¶**
1. `search_arxiv` æœç´¢ç›¸å…³è®ºæ–‡
2. `download_arxiv` ä¸‹è½½æ„Ÿå…´è¶£çš„è®ºæ–‡
3. `ingest_knowledge` å°†è®ºæ–‡åŠ å…¥çŸ¥è¯†åº“
4. `query_knowledge_base` å›ç­”ç”¨æˆ·å…·ä½“é—®é¢˜
5. `format_paper_analysis` ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š

**ç¤ºä¾‹3: ç¤¾äº¤åª’ä½“äººç‰©è°ƒç ”**
1. `web_data_linkedin_person_profile` è·å–èŒä¸šèƒŒæ™¯
2. `web_data_instagram_profiles` è·å–ç¤¾äº¤åŠ¨æ€
3. `web_data_x_posts` è·å–å…¬å¼€è¨€è®º
4. ç»¼åˆåˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š

**ç¤ºä¾‹4: æ•°æ®åˆ†æä»»åŠ¡** ğŸ†•
1. ç”¨æˆ·ä¸Šä¼  sales.csv
2. `upload_data_to_sandbox("sales.csv")` ä¸Šä¼ åˆ°æ²™ç®±
3. `analyze_csv_data("/home/user/data/sales.csv", "åˆ†æé”€å”®è¶‹åŠ¿")` æ•°æ®æ¦‚è§ˆ
4. `execute_python_code(detailed_analysis)` æ·±åº¦åˆ†æ
5. `generate_chart_from_data("sales.csv", "month", "revenue", "line", "æœˆåº¦é”€å”®è¶‹åŠ¿")` ç”Ÿæˆå›¾è¡¨

**ç¤ºä¾‹5: ä»£ç éªŒè¯ä»»åŠ¡** ğŸ†•
1. ç”¨æˆ·: "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå¹¶éªŒè¯"
2. ç”Ÿæˆå¿«é€Ÿæ’åºä»£ç 
3. `execute_python_code(quicksort_with_tests)` è¿è¡Œå¹¶éªŒè¯
4. è¿”å›æ‰§è¡Œç»“æœå’Œæµ‹è¯•è¾“å‡º

---

## ğŸ“‹ è¡ŒåŠ¨æŒ‡å— (ReActæ€è€ƒæ¨¡å¼)

1. **åˆ†æè¯·æ±‚**: è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼Œç¡®å®šæ‰€éœ€å·¥å…·ç±»åˆ«
2. **è§„åˆ’å·¥å…·é“¾**: å¤æ‚ä»»åŠ¡éœ€è¦å¤šæ­¥éª¤ï¼Œå…ˆè§„åˆ’å†æ‰§è¡Œ
3. **æ‰§è¡Œå¹¶éªŒè¯**: è°ƒç”¨å·¥å…·è·å–æ•°æ®ï¼Œæ£€æŸ¥ç»“æœå®Œæ•´æ€§
4. **ç»¼åˆå›ç­”**: æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç»™å‡ºç»“æ„åŒ–çš„æœ€ç»ˆç­”æ¡ˆ

**ç‰¹åˆ«æ³¨æ„**:
- ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶ â†’ è‡ªåŠ¨è°ƒç”¨ `ingest_knowledge(filename, 'file')`
- ç”¨æˆ·å‘é€URL â†’ åˆ¤æ–­æ˜¯å¦éœ€è¦å­¦ä¹  `ingest_knowledge` è¿˜æ˜¯ç›´æ¥æŠ“å– `scrape_as_markdown`
- ç”¨æˆ·é—®"åˆšæ‰çš„æ–‡ä»¶" â†’ æ£€æŸ¥ä¸Šä¸‹æ–‡è·å–æ–‡ä»¶åï¼Œä½¿ç”¨ `query_knowledge_base`
- å¯¹äºRAGä»»åŠ¡ï¼Œä¼˜å…ˆä½¿ç”¨ `source_filter` ç²¾ç¡®æŸ¥è¯¢ï¼Œæ— ç»“æœå†å…¨å±€æŸ¥è¯¢
- **æ•°æ®æ–‡ä»¶åˆ†æ** â†’ å…ˆ `upload_data_to_sandbox`ï¼Œå†ç”¨ä»£ç åˆ†æå·¥å…·å¤„ç†
- **éœ€è¦æ‰§è¡Œä»£ç ** â†’ ä½¿ç”¨ `execute_python_code`ï¼Œä»£ç åœ¨å®‰å…¨æ²™ç®±ä¸­è¿è¡Œ
"""

async def initialize_agent(api_keys: Dict[str, str] = None):
    """
    Initialize the LangGraph agent with MCP tools, custom tools, and SQLite persistence.
    """
    global _agent_executor, _mcp_client, _mcp_tools, _sqlite_conn

    print("ğŸš€ [Agent Service] Initializing Agent with Persistence...")
    
    # 1. Configure MCP Client (Same as before)
    mcp_servers = {}
    bd_key = api_keys.get("BRIGHT_DATA_API_KEY") if api_keys else os.environ.get("BRIGHT_DATA_API_KEY")
    if bd_key:
        mcp_servers["bright_data"] = {
            "url": f"https://mcp.brightdata.com/mcp?token={bd_key}&pro=1",
            "transport": "streamable_http",
        }
    ps_key = api_keys.get("PAPER_SEARCH_API_KEY") if api_keys else os.environ.get("PAPER_SEARCH_API_KEY")
    if ps_key:
        mcp_servers["paper_search"] = {
            "url": f"https://server.smithery.ai/@adamamer20/paper-search-mcp-openai/mcp?api_key={ps_key}",
            "transport": "streamable_http",
        }

    custom_tools = [
        # generate_search_queries,
        # execute_searches_and_get_urls,
        ingest_knowledge,
        query_knowledge_base,
        format_paper_analysis,
        format_linkedin_profile,
        # E2B Code Interpreter tools (simplified - only core tools)
        execute_python_code,  # Main tool for code execution and visualization
        execute_shell_command,
        install_python_package,
        upload_data_to_sandbox,
        download_file_from_sandbox,
        analyze_csv_data,
        # Removed: generate_chart_from_data, create_visualization
        # Reason: execute_python_code handles all these cases better
    ]

    if mcp_servers:
        try:
            _mcp_client = MultiServerMCPClient(mcp_servers)
            try:
                _mcp_tools = await _mcp_client.get_tools()
                print(f"âœ… [Agent Service] Loaded {len(_mcp_tools)} MCP tools.")
            except Exception as e:
                print(f"âš ï¸ [Agent Service] Failed to load MCP tools: {e}")
                _mcp_tools = []
        except Exception as e:
            print(f"âš ï¸ [Agent Service] Failed to connect to MCP servers: {e}")
            _mcp_tools = []
    else:
        _mcp_tools = []

    all_tools = _mcp_tools + custom_tools


    print(f"âœ… [Agent Service] Loaded {len(all_tools)} total tools.")

    print(f"âœ… [Agent Service] Loaded {all_tools} ")

    # 2. Configure LLM (æ”¯æŒä¸¤ç§æ¨¡å¼)
    llm_provider = api_keys.get("LLM_PROVIDER", "google") if api_keys else os.environ.get("LLM_PROVIDER", "google")

    if llm_provider == "openai_compatible":
        # ä½¿ç”¨ OpenAI å…¼å®¹çš„ç¬¬ä¸‰æ–¹ä¸­è½¬å¹³å°
        openai_base_url = api_keys.get("OPENAI_BASE_URL") if api_keys else os.environ.get("OPENAI_BASE_URL")
        openai_api_key = api_keys.get("OPENAI_API_KEY") if api_keys else os.environ.get("OPENAI_API_KEY")
        openai_model = api_keys.get("OPENAI_MODEL", "gpt-4o-mini") if api_keys else os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

        if not openai_base_url or not openai_api_key:
            raise ValueError("OpenAI Compatible mode requires OPENAI_BASE_URL and OPENAI_API_KEY!")

        llm = ChatOpenAI(
            model=openai_model,
            base_url=openai_base_url,
            api_key=openai_api_key,
            temperature=0
        )
        print(f"âœ… [Agent Service] Using OpenAI Compatible LLM: {openai_model} @ {openai_base_url}")
    else:
        # é»˜è®¤ä½¿ç”¨ Google Gemini å®˜æ–¹ API
        if "GOOGLE_API_KEY" not in os.environ:
            raise ValueError("GOOGLE_API_KEY is missing!")

        google_model = api_keys.get("GOOGLE_MODEL", "gemini-2.0-flash-lite") if api_keys else os.environ.get("GOOGLE_MODEL", "gemini-2.0-flash-lite")

        llm = ChatGoogleGenerativeAI(
            model=google_model,
            google_api_key=os.environ["GOOGLE_API_KEY"],
            temperature=0
        )
        print(f"âœ… [Agent Service] Using Google Gemini LLM: {google_model}")

    # 3. Create LangGraph Agent with AsyncSqliteSaver
    if _sqlite_conn is None:
        _sqlite_conn = await aiosqlite.connect(DB_PATH)
        
    checkpointer = AsyncSqliteSaver(_sqlite_conn)
    
    _agent_executor = create_react_agent(
        model=llm,
        tools=all_tools,
        checkpointer=checkpointer
    )
    
    print("âœ… [Agent Service] Persistent Agent initialized successfully.")
    return _agent_executor

async def get_agent_executor():
    global _agent_executor
    if _agent_executor is None:
        await initialize_agent()
    return _agent_executor

async def chat_with_agent(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Main entry point for chatting (Synchronous return for now, will be upgraded to stream).
    """
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    config = {"configurable": {"thread_id": thread_id}}
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]
    
    final_state = await agent.ainvoke(
        {"messages": messages},
        config=config
    )
    
    return final_state["messages"][-1].content

async def chat_with_agent_stream(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Generator function for streaming agent responses and thoughts.
    Uses base64 encoding to ensure SSE data integrity (handles newlines in content).
    """
    import base64
    import json
    
    def encode_sse_data(data: str) -> str:
        """Base64 encode data to avoid SSE newline issues."""
        return base64.b64encode(data.encode('utf-8')).decode('ascii')
    
    def extract_text_content(content) -> str:
        """
        Extract text from Gemini's chunk.content which may be:
        - A simple string
        - A list of content parts (e.g., [{'type': 'text', 'text': '...'}])
        - None or empty
        """
        if content is None:
            return ""
        
        if isinstance(content, str):
            return content
        
        if isinstance(content, list):
            # Handle list of content parts (Gemini format)
            text_parts = []
            for part in content:
                if isinstance(part, str):
                    text_parts.append(part)
                elif isinstance(part, dict):
                    # Extract text from dict-like content part
                    if 'text' in part:
                        text_parts.append(part['text'])
                    elif 'content' in part:
                        text_parts.append(str(part['content']))
            return ''.join(text_parts)
        
        # Fallback: convert to string
        return str(content) if content else ""
    
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    config = {"configurable": {"thread_id": thread_id}}
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]

    async for event in agent.astream_events({"messages": messages}, config=config, version="v1"):
        kind = event["event"]
        
        # Yield different event types for the frontend to consume
        # All data is base64 encoded to handle newlines safely
        if kind == "on_chat_model_stream":
            raw_content = event["data"]["chunk"].content
            text_content = extract_text_content(raw_content)
            if text_content:
                encoded_data = encode_sse_data(text_content)
                yield f"event: text\ndata: {encoded_data}\n\n"
        
        elif kind == "on_tool_start":
            tool_name = event["name"]
            encoded_data = encode_sse_data(tool_name)
            yield f"event: tool_start\ndata: {encoded_data}\n\n"
            
        elif kind == "on_tool_end":
            tool_name = event["name"]
            output = str(event["data"].get("output", ""))

            # Check if output contains image data - don't truncate images!
            if "[IMAGE_BASE64:" in output:
                # Extract and preserve image data, truncate only non-image text
                import re
                image_pattern = r'\[IMAGE_BASE64:[A-Za-z0-9+/=]+\]'
                images = re.findall(image_pattern, output)
                text_parts = re.split(image_pattern, output)

                # Truncate text parts but keep full images
                truncated_text_parts = [
                    (part[:500] + '...' if len(part) > 500 else part)
                    for part in text_parts
                ]

                # Reconstruct output with full images
                safe_output = ""
                for i, text_part in enumerate(truncated_text_parts):
                    safe_output += text_part
                    if i < len(images):
                        safe_output += images[i]
            else:
                # No images - apply normal truncation
                safe_output = (output[:1000] + '...') if len(output) > 1000 else output

            tool_data = json.dumps({"name": tool_name, "output": safe_output}, ensure_ascii=False)
            encoded_data = encode_sse_data(tool_data)
            yield f"event: tool_end\ndata: {encoded_data}\n\n"
    
    # Send stream end marker
    yield f"event: done\ndata: complete\n\n"


async def cleanup():
    """Cleanup function to close database connection and E2B sandbox."""
    global _sqlite_conn, _mcp_client
    if _sqlite_conn:
        await _sqlite_conn.close()
        _sqlite_conn = None
    if _mcp_client:
        _mcp_client = None
    # æ¸…ç† E2B æ²™ç®±
    await close_sandbox()
