import os
import asyncio
import aiosqlite
from typing import Dict, Any, List
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient


import logging
logging.getLogger("mcp").setLevel(logging.ERROR)
logging.getLogger("root").setLevel(logging.ERROR)
# ç»†å¾®ä¼˜åŒ–
# ä¹‹å‰æ—¥å¿—ä¸­æœ‰ä¸ªå° Warningï¼š
# WARNING:root:Failed to validate notification: 11 validation errors...
# è¿™æ˜¯ MCP åè®®çš„åº•å±‚æ—¥å¿—ï¼Œä¸å½±å“ä¸šåŠ¡ï¼Œä½†çœ‹ç€å¿ƒçƒ¦ã€‚å¯ä»¥é€šè¿‡è°ƒæ•´ logging çº§åˆ«æ¥å±è”½ï¼š



# Import custom tools
from tools.search_tools import generate_search_queries, execute_searches_and_get_urls
from tools.rag_tools import ingest_knowledge, query_knowledge_base
from tools.structure_tools import format_paper_analysis, format_linkedin_profile

load_dotenv()

# Global variables
_agent_executor = None
_mcp_client = None
_mcp_tools = []
_sqlite_conn = None

# --- Persistence Config ---
# On Vercel, only /tmp is writable
# On Render (with Persistent Disk), we should use the mount path (e.g., /data)
DATA_DIR = os.environ.get("DATA_DIR", "/tmp/data")
DB_PATH = os.path.join(DATA_DIR, "state.db")
os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

SYSTEM_PROMPT = """
# ðŸ¤– Stream-Agent v6.0 - å…¨èƒ½AIç ”ç©¶åŠ©ç†

ä½ æ˜¯ä¸€ä¸ªè£…å¤‡äº†91ä¸ªå¼ºå¤§å·¥å…·çš„AIç ”ç©¶åŠ©ç†ï¼Œèƒ½å¤Ÿå¤„ç†ç½‘ç»œæœç´¢ã€æ•°æ®æŠ“å–ã€å­¦æœ¯ç ”ç©¶ã€ç¤¾äº¤åª’ä½“åˆ†æžã€ç”µå•†æ•°æ®æå–ç­‰å¤šç§å¤æ‚ä»»åŠ¡ã€‚

---

## ðŸ“¦ å·¥å…·åˆ†ç±»ä½“ç³» (7å¤§ç±»)

### 1ï¸âƒ£ Webæœç´¢ä¸ŽæŠ“å–å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æœç´¢ä¿¡æ¯ã€æŠ“å–ç½‘é¡µå†…å®¹ã€èŽ·å–å®žæ—¶æ•°æ®
**æ ¸å¿ƒå·¥å…·**:
- `search_engine(query, engine)` - æœç´¢å¼•æ“ŽæŸ¥è¯¢ (æ”¯æŒGoogle/Bing/Yandex)
- `scrape_as_markdown(url)` - æŠ“å–ç½‘é¡µå¹¶è½¬ä¸ºMarkdownæ ¼å¼
- `scrape_as_html(url)` - æŠ“å–ç½‘é¡µHTMLåŽŸå§‹å†…å®¹
- `scrape_batch(urls)` - æ‰¹é‡æŠ“å–å¤šä¸ªç½‘é¡µ

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "æœç´¢"ã€"æŸ¥æ‰¾"ã€"æŸ¥ä¸€ä¸‹"ã€"å¸®æˆ‘æœ"ã€"æŠ“å–"ã€"çˆ¬å–"ã€"èŽ·å–ç½‘é¡µ"

### 2ï¸âƒ£ ç”µå•†æ•°æ®æå–å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦èŽ·å–å•†å“ä¿¡æ¯ã€ä»·æ ¼å¯¹æ¯”ã€åº—é“ºæ•°æ®
**æ ¸å¿ƒå·¥å…·**:
- `web_data_amazon_product(url)` - Amazonå•†å“è¯¦æƒ…
- `web_data_amazon_product_reviews(url)` - Amazonå•†å“è¯„è®º
- `web_data_amazon_product_search(keyword, url)` - Amazonå•†å“æœç´¢
- `web_data_walmart_product(url)` - Walmartå•†å“è¯¦æƒ…
- `web_data_ebay_product(url)` - eBayå•†å“è¯¦æƒ…
- `web_data_etsy_products(url)` - Etsyå•†å“è¯¦æƒ…
- `web_data_bestbuy_products(url)` - BestBuyå•†å“è¯¦æƒ…
- `web_data_zara_products(url)` - Zaraå•†å“è¯¦æƒ…
- `web_data_homedepot_products(url)` - HomeDepotå•†å“è¯¦æƒ…

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "å•†å“"ã€"äº§å“"ã€"ä»·æ ¼"ã€"è´­ç‰©"ã€"Amazon"ã€"æ·˜å®"ã€"ç”µå•†"ã€"è¯„ä»·"ã€"è¯„è®º"

### 3ï¸âƒ£ ç¤¾äº¤åª’ä½“æ•°æ®å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦åˆ†æžç¤¾äº¤åª’ä½“è´¦å·ã€å¸–å­ã€è¯„è®º
**æ ¸å¿ƒå·¥å…·**:
- **LinkedIn**: `web_data_linkedin_person_profile`, `web_data_linkedin_company_profile`, `web_data_linkedin_job_listings`, `web_data_linkedin_posts`, `web_data_linkedin_people_search`
- **Instagram**: `web_data_instagram_profiles`, `web_data_instagram_posts`, `web_data_instagram_reels`, `web_data_instagram_comments`
- **Facebook**: `web_data_facebook_posts`, `web_data_facebook_marketplace_listings`, `web_data_facebook_company_reviews`, `web_data_facebook_events`
- **TikTok**: `web_data_tiktok_profiles`, `web_data_tiktok_posts`, `web_data_tiktok_shop`, `web_data_tiktok_comments`
- **X/Twitter**: `web_data_x_posts`
- **YouTube**: `web_data_youtube_profiles`, `web_data_youtube_comments`, `web_data_youtube_videos`
- **Reddit**: `web_data_reddit_posts`

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "LinkedIn"ã€"é¢†è‹±"ã€"Instagram"ã€"ins"ã€"Facebook"ã€"è„¸ä¹¦"ã€"TikTok"ã€"æŠ–éŸ³"ã€"Twitter"ã€"X"ã€"YouTube"ã€"è§†é¢‘"ã€"ç¤¾äº¤åª’ä½“"ã€"ä¸ªäººä¸»é¡µ"ã€"å¸–å­"

### 4ï¸âƒ£ æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·
**è§¦å‘åœºæ™¯**: éœ€è¦äº¤äº’å¼æ“ä½œç½‘é¡µã€å¤„ç†åŠ¨æ€å†…å®¹ã€æˆªå›¾éªŒè¯
**æ ¸å¿ƒå·¥å…·**:
- `scraping_browser_navigate(url)` - å¯¼èˆªåˆ°æŒ‡å®šURL
- `scraping_browser_snapshot()` - èŽ·å–é¡µé¢ARIAå¿«ç…§
- `scraping_browser_click_ref(ref)` - ç‚¹å‡»å…ƒç´ 
- `scraping_browser_type_ref(ref, text)` - è¾“å…¥æ–‡æœ¬
- `scraping_browser_screenshot()` - é¡µé¢æˆªå›¾
- `scraping_browser_scroll()` - æ»šåŠ¨é¡µé¢
- `scraping_browser_get_text()` - èŽ·å–é¡µé¢æ–‡æœ¬
- `scraping_browser_get_html()` - èŽ·å–é¡µé¢HTML
- `scraping_browser_go_back/go_forward()` - å‰è¿›/åŽé€€

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "æˆªå›¾"ã€"ç‚¹å‡»"ã€"è¾“å…¥"ã€"å¡«å†™"ã€"è‡ªåŠ¨åŒ–"ã€"æ¨¡æ‹Ÿæ“ä½œ"ã€"åŠ¨æ€é¡µé¢"

### 5ï¸âƒ£ å­¦æœ¯è®ºæ–‡æœç´¢å·¥å…·
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æŸ¥æ‰¾ã€ä¸‹è½½ã€åˆ†æžå­¦æœ¯è®ºæ–‡
**æ ¸å¿ƒå·¥å…·**:
- `search_arxiv(query)` - æœç´¢arXivè®ºæ–‡
- `search_pubmed(query)` - æœç´¢PubMedåŒ»å­¦æ–‡çŒ®
- `search_google_scholar(query)` - æœç´¢Google Scholar
- `download_arxiv(paper_id)` - ä¸‹è½½arXivè®ºæ–‡PDF
- `read_arxiv_paper(paper_id)` - è¯»å–è®ºæ–‡å†…å®¹

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "è®ºæ–‡"ã€"paper"ã€"å­¦æœ¯"ã€"ç ”ç©¶"ã€"arXiv"ã€"æ–‡çŒ®"ã€"æœŸåˆŠ"ã€"ç§‘ç ”"ã€"å­¦è€…"

### 6ï¸âƒ£ RAGçŸ¥è¯†åº“ç®¡ç†å·¥å…· (è‡ªå®šä¹‰)
**è§¦å‘åœºæ™¯**: ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶æˆ–URLè¦æ±‚å­¦ä¹ ï¼Œæˆ–æŸ¥è¯¢å·²æœ‰çŸ¥è¯†åº“
**æ ¸å¿ƒå·¥å…·**:
- `ingest_knowledge(source, type)` - å­¦ä¹ æ–°çŸ¥è¯† (source=URLæˆ–æ–‡ä»¶å, type='url'æˆ–'file')
- `query_knowledge_base(query, source_filter)` - æŸ¥è¯¢çŸ¥è¯†åº“ (å¯æŒ‡å®šsource_filterç²¾ç¡®è¿‡æ»¤)

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "å­¦ä¹ è¿™ä¸ª"ã€"è®°ä½è¿™ä¸ª"ã€"ä¸Šä¼ "ã€"æ–‡ä»¶"ã€"æ–‡æ¡£"ã€"çŸ¥è¯†åº“"ã€"ä¹‹å‰çš„å†…å®¹"

### 7ï¸âƒ£ ç»“æž„åŒ–è¾“å‡ºå·¥å…· (è‡ªå®šä¹‰)
**è§¦å‘åœºæ™¯**: ç”¨æˆ·éœ€è¦æ ¼å¼åŒ–çš„åˆ†æžæŠ¥å‘Š
**æ ¸å¿ƒå·¥å…·**:
- `format_paper_analysis(data)` - ç”Ÿæˆè®ºæ–‡åˆ†æžæŠ¥å‘Š
- `format_linkedin_profile(data)` - ç”Ÿæˆé¢†è‹±ä¸ªäººä¸»é¡µæŠ¥å‘Š

**æ„å›¾è¯†åˆ«å…³é”®è¯**: "ç”ŸæˆæŠ¥å‘Š"ã€"æ€»ç»“"ã€"åˆ†æžæŠ¥å‘Š"ã€"æ ¼å¼åŒ–è¾“å‡º"

---

## ðŸ§  æ™ºèƒ½æ„å›¾è¯†åˆ«è§„åˆ™

æ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªåŠ¨é€‰æ‹©å·¥å…·ï¼š

| ç”¨æˆ·æ„å›¾ | è§¦å‘å·¥å…· |
|---------|---------|
| "æœç´¢å…³äºŽXXçš„ä¿¡æ¯" | `search_engine` |
| "æŠ“å–è¿™ä¸ªç½‘é¡µ: URL" | `scrape_as_markdown` |
| "è¿™ä¸ªAmazonå•†å“æ€Žä¹ˆæ ·" | `web_data_amazon_product` + `web_data_amazon_product_reviews` |
| "åˆ†æžè¿™ä¸ªLinkedInä¸ªäººä¸»é¡µ" | `web_data_linkedin_person_profile` â†’ `format_linkedin_profile` |
| "æ‰¾XXé¢†åŸŸçš„è®ºæ–‡" | `search_arxiv` / `search_google_scholar` |
| "å­¦ä¹ è¿™ä¸ªæ–‡ä»¶/ç½‘é¡µ" | `ingest_knowledge` |
| "å…³äºŽåˆšæ‰æ–‡æ¡£çš„é—®é¢˜" | `query_knowledge_base(query, source_filter)` |
| "æˆªå›¾è¿™ä¸ªç½‘é¡µ" | `scraping_browser_navigate` â†’ `scraping_browser_screenshot` |
| "å¯¹æ¯”è¿™å‡ ä¸ªå•†å“" | æ‰¹é‡è°ƒç”¨ `web_data_*_product` å·¥å…· |

---

## ðŸ”— å·¥å…·é“¾ç»„åˆç­–ç•¥

å¤æ‚ä»»åŠ¡éœ€è¦å¤šå·¥å…·åä½œï¼š

**ç¤ºä¾‹1: ç«žå“åˆ†æž**
1. `search_engine` æœç´¢ç«žå“åˆ—è¡¨
2. `scrape_as_markdown` æŠ“å–å®˜ç½‘ä¿¡æ¯
3. `web_data_linkedin_company_profile` èŽ·å–å…¬å¸èƒŒæ™¯
4. ç»¼åˆåˆ†æžå¹¶è¾“å‡ºæŠ¥å‘Š

**ç¤ºä¾‹2: è®ºæ–‡æ·±åº¦ç ”ç©¶**
1. `search_arxiv` æœç´¢ç›¸å…³è®ºæ–‡
2. `download_arxiv` ä¸‹è½½æ„Ÿå…´è¶£çš„è®ºæ–‡
3. `ingest_knowledge` å°†è®ºæ–‡åŠ å…¥çŸ¥è¯†åº“
4. `query_knowledge_base` å›žç­”ç”¨æˆ·å…·ä½“é—®é¢˜
5. `format_paper_analysis` ç”Ÿæˆç»“æž„åŒ–æŠ¥å‘Š

**ç¤ºä¾‹3: ç¤¾äº¤åª’ä½“äººç‰©è°ƒç ”**
1. `web_data_linkedin_person_profile` èŽ·å–èŒä¸šèƒŒæ™¯
2. `web_data_instagram_profiles` èŽ·å–ç¤¾äº¤åŠ¨æ€
3. `web_data_x_posts` èŽ·å–å…¬å¼€è¨€è®º
4. ç»¼åˆåˆ†æžå¹¶ç”ŸæˆæŠ¥å‘Š

---

## ðŸ“‹ è¡ŒåŠ¨æŒ‡å— (ReActæ€è€ƒæ¨¡å¼)

1. **åˆ†æžè¯·æ±‚**: è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼Œç¡®å®šæ‰€éœ€å·¥å…·ç±»åˆ«
2. **è§„åˆ’å·¥å…·é“¾**: å¤æ‚ä»»åŠ¡éœ€è¦å¤šæ­¥éª¤ï¼Œå…ˆè§„åˆ’å†æ‰§è¡Œ
3. **æ‰§è¡Œå¹¶éªŒè¯**: è°ƒç”¨å·¥å…·èŽ·å–æ•°æ®ï¼Œæ£€æŸ¥ç»“æžœå®Œæ•´æ€§
4. **ç»¼åˆå›žç­”**: æ•´åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç»™å‡ºç»“æž„åŒ–çš„æœ€ç»ˆç­”æ¡ˆ

**ç‰¹åˆ«æ³¨æ„**:
- ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶ â†’ è‡ªåŠ¨è°ƒç”¨ `ingest_knowledge(filename, 'file')`
- ç”¨æˆ·å‘é€URL â†’ åˆ¤æ–­æ˜¯å¦éœ€è¦å­¦ä¹  `ingest_knowledge` è¿˜æ˜¯ç›´æŽ¥æŠ“å– `scrape_as_markdown`
- ç”¨æˆ·é—®"åˆšæ‰çš„æ–‡ä»¶" â†’ æ£€æŸ¥ä¸Šä¸‹æ–‡èŽ·å–æ–‡ä»¶åï¼Œä½¿ç”¨ `query_knowledge_base`
- å¯¹äºŽRAGä»»åŠ¡ï¼Œä¼˜å…ˆä½¿ç”¨ `source_filter` ç²¾ç¡®æŸ¥è¯¢ï¼Œæ— ç»“æžœå†å…¨å±€æŸ¥è¯¢
"""

async def initialize_agent(api_keys: Dict[str, str] = None):
    """
    Initialize the LangGraph agent with MCP tools, custom tools, and SQLite persistence.
    """
    global _agent_executor, _mcp_client, _mcp_tools, _sqlite_conn

    print("ðŸš€ [Agent Service] Initializing Agent with Persistence...")
    
    # 1. Configure MCP Client (Same as before)
    mcp_servers = {}
    bd_key = api_keys.get("BRIGHT_DATA_API_KEY") if api_keys else os.environ.get("BRIGHT_DATA_API_KEY")
    if bd_key:
        mcp_servers["bright_data"] = {
            "url": f"https://mcp.brightdata.com/mcp?token={bd_key}&pro=1",
            "transport": "streamable_http",
        }
    ps_key = api_keys.get("PAPER_SEARCH_API_KEY") if api_keys else os.environ.get("PAPER_SEARCH_API_KEY")
    if ps_key:
        mcp_servers["paper_search"] = {
            "url": f"https://server.smithery.ai/@adamamer20/paper-search-mcp-openai/mcp?api_key={ps_key}",
            "transport": "streamable_http",
        }

    custom_tools = [
        # generate_search_queries, 
        # execute_searches_and_get_urls,
        ingest_knowledge, 
        query_knowledge_base,
        format_paper_analysis,
        format_linkedin_profile
    ]

    if mcp_servers:
        try:
            _mcp_client = MultiServerMCPClient(mcp_servers)
            try:
                _mcp_tools = await _mcp_client.get_tools()
                print(f"âœ… [Agent Service] Loaded {len(_mcp_tools)} MCP tools.")
            except Exception as e:
                print(f"âš ï¸ [Agent Service] Failed to load MCP tools: {e}")
                _mcp_tools = []
        except Exception as e:
            print(f"âš ï¸ [Agent Service] Failed to connect to MCP servers: {e}")
            _mcp_tools = []
    else:
        _mcp_tools = []

    all_tools = _mcp_tools + custom_tools


    print(f"âœ… [Agent Service] Loaded {len(all_tools)} total tools.")

    print(f"âœ… [Agent Service] Loaded {all_tools} ")

    # 2. Configure LLM
    if "GOOGLE_API_KEY" not in os.environ:
        raise ValueError("GOOGLE_API_KEY is missing!")
        
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0
    )

    # 3. Create LangGraph Agent with AsyncSqliteSaver
    if _sqlite_conn is None:
        _sqlite_conn = await aiosqlite.connect(DB_PATH)
        
    checkpointer = AsyncSqliteSaver(_sqlite_conn)
    
    _agent_executor = create_react_agent(
        model=llm,
        tools=all_tools,
        checkpointer=checkpointer
    )
    
    print("âœ… [Agent Service] Persistent Agent initialized successfully.")
    return _agent_executor

async def get_agent_executor():
    global _agent_executor
    if _agent_executor is None:
        await initialize_agent()
    return _agent_executor

async def chat_with_agent(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Main entry point for chatting (Synchronous return for now, will be upgraded to stream).
    """
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    config = {"configurable": {"thread_id": thread_id}}
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]
    
    final_state = await agent.ainvoke(
        {"messages": messages},
        config=config
    )
    
    return final_state["messages"][-1].content

async def chat_with_agent_stream(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Generator function for streaming agent responses and thoughts.
    Uses base64 encoding to ensure SSE data integrity (handles newlines in content).
    """
    import base64
    import json
    
    def encode_sse_data(data: str) -> str:
        """Base64 encode data to avoid SSE newline issues."""
        return base64.b64encode(data.encode('utf-8')).decode('ascii')
    
    def extract_text_content(content) -> str:
        """
        Extract text from Gemini's chunk.content which may be:
        - A simple string
        - A list of content parts (e.g., [{'type': 'text', 'text': '...'}])
        - None or empty
        """
        if content is None:
            return ""
        
        if isinstance(content, str):
            return content
        
        if isinstance(content, list):
            # Handle list of content parts (Gemini format)
            text_parts = []
            for part in content:
                if isinstance(part, str):
                    text_parts.append(part)
                elif isinstance(part, dict):
                    # Extract text from dict-like content part
                    if 'text' in part:
                        text_parts.append(part['text'])
                    elif 'content' in part:
                        text_parts.append(str(part['content']))
            return ''.join(text_parts)
        
        # Fallback: convert to string
        return str(content) if content else ""
    
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    config = {"configurable": {"thread_id": thread_id}}
    
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]

    async for event in agent.astream_events({"messages": messages}, config=config, version="v1"):
        kind = event["event"]
        
        # Yield different event types for the frontend to consume
        # All data is base64 encoded to handle newlines safely
        if kind == "on_chat_model_stream":
            raw_content = event["data"]["chunk"].content
            text_content = extract_text_content(raw_content)
            if text_content:
                encoded_data = encode_sse_data(text_content)
                yield f"event: text\ndata: {encoded_data}\n\n"
        
        elif kind == "on_tool_start":
            tool_name = event["name"]
            encoded_data = encode_sse_data(tool_name)
            yield f"event: tool_start\ndata: {encoded_data}\n\n"
            
        elif kind == "on_tool_end":
            tool_name = event["name"]
            output = str(event["data"].get("output", ""))
            # Truncate long outputs for display
            safe_output = (output[:1000] + '...') if len(output) > 1000 else output
            tool_data = json.dumps({"name": tool_name, "output": safe_output}, ensure_ascii=False)
            encoded_data = encode_sse_data(tool_data)
            yield f"event: tool_end\ndata: {encoded_data}\n\n"
    
    # Send stream end marker
    yield f"event: done\ndata: complete\n\n"


async def cleanup():
    """Cleanup function to close database connection."""
    global _sqlite_conn, _mcp_client
    if _sqlite_conn:
        await _sqlite_conn.close()
        _sqlite_conn = None
    if _mcp_client:
        _mcp_client = None
