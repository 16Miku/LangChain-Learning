import os
import asyncio
from typing import Dict, Any, List
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient

# Import custom tools
from tools.search_tools import generate_search_queries, execute_searches_and_get_urls
from tools.rag_tools import ingest_knowledge, query_knowledge
from tools.structure_tools import format_paper_analysis, format_linkedin_profile

load_dotenv()

# Global variables to hold the initialized agent and client
_agent_executor = None
_mcp_client = None
_mcp_tools = []

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½çš„ AI ç ”ç©¶åŠ©ç† (My-Chat-LangChain v5.0)ã€‚
ä½ å¯ä»¥å¤„ç†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†æå­¦æœ¯è®ºæ–‡ã€æŸ¥è¯¢ä¸ªäººèµ„æ–™ã€æ‰§è¡Œå¤æ‚çš„ç½‘ç»œæœç´¢ï¼Œä»¥åŠæ·±å…¥å­¦ä¹ å’ŒæŸ¥è¯¢ç‰¹å®šçš„ç½‘é¡µçŸ¥è¯†åº“ã€‚

**ä½ çš„èƒ½åŠ› (å·¥å…·ç®±):**
1.  **RAG çŸ¥è¯†åº“å·¥å…·**:
    *   `ingest_knowledge(url)`: å½“ç”¨æˆ·è¦æ±‚å­¦ä¹ æŸä¸ªæ–°ç½‘é¡µï¼Œæˆ–åŸºäºæŸä¸ªç½‘é¡µå›ç­”é—®é¢˜æ—¶ï¼Œ**å¿…é¡»**å…ˆè°ƒç”¨æ­¤å·¥å…·ã€‚
    *   `query_knowledge(query, url)`: å½“éœ€è¦ä»å·²å­¦ä¹ çš„ç½‘é¡µä¸­æ£€ç´¢è¯¦ç»†ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚
2.  **æœç´¢ä¸åˆ†æå·¥å…·**:
    *   `generate_search_queries`: ç”Ÿæˆä¸“ä¸šçš„æœç´¢ç­–ç•¥ã€‚
    *   `execute_searches_and_get_urls`: æ‰§è¡Œæœç´¢å¹¶è·å– URLã€‚
    *   ä»¥åŠæ¥è‡ª MCP (å¦‚ BrightData, PaperSearch) çš„å…¶ä»–å¼ºå¤§å·¥å…·ï¼ˆå¦‚æœå·²é…ç½®ï¼‰ã€‚
3.  **ç»“æ„åŒ–æŠ¥å‘Šå·¥å…·**:
    *   `format_paper_analysis`: å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚å¯¹è®ºæ–‡è¿›è¡Œåˆ†ææŠ¥å‘Šæ—¶ï¼Œåœ¨æ”¶é›†å®Œä¿¡æ¯åè°ƒç”¨æ­¤å·¥å…·è¾“å‡ºç»“æœã€‚
    *   `format_linkedin_profile`: å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚æå–é¢†è‹±ä¸ªäººä¸»é¡µä¿¡æ¯æ—¶ï¼Œåœ¨æ”¶é›†å®Œä¿¡æ¯åè°ƒç”¨æ­¤å·¥å…·è¾“å‡ºç»“æœã€‚

**ä½ çš„è¡ŒåŠ¨æŒ‡å— (ReAct æ€è€ƒæ¨¡å¼):**
1.  **åˆ†æä¸è§„åˆ’**: ä»”ç»†é˜…è¯»ç”¨æˆ·çš„è¯·æ±‚ã€‚
    *   å¦‚æœæ˜¯å…³äºç‰¹å®šç½‘é¡µçš„é—®ç­” -> 1. `ingest_knowledge` -> 2. `query_knowledge`ã€‚
    *   å¦‚æœæ˜¯ç”ŸæˆæŠ¥å‘Š -> æ”¶é›†ä¿¡æ¯ -> è°ƒç”¨ `format_paper_analysis` æˆ– `format_linkedin_profile`ã€‚
2.  **ä¿¡æ¯æ”¶é›†**: çµæ´»ç»„åˆä½¿ç”¨ä½ çš„å·¥å…·ã€‚
3.  **ç”Ÿæˆå›ç­”**: ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚å¦‚æœç”¨æˆ·åªæ˜¯é—²èŠï¼Œç›´æ¥å›ç­”å³å¯ã€‚

**æ³¨æ„äº‹é¡¹**:
*   åœ¨å›ç­”ä¹‹å‰ï¼Œä»”ç»†æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ç»“æ„åŒ–å·¥å…·é€‚åˆå½“å‰ä»»åŠ¡ã€‚
*   å¯¹äº RAG ä»»åŠ¡ï¼Œç¡®ä¿ URL å‡†ç¡®æ— è¯¯ã€‚
"""

async def initialize_agent(api_keys: Dict[str, str] = None):
    """
    Initialize the LangGraph agent with MCP tools and custom tools.
    This can be re-called if API keys are updated dynamically.
    """
    global _agent_executor, _mcp_client, _mcp_tools

    print("ğŸš€ [Agent Service] Initializing Agent...")
    
    # 1. Configure MCP Client
    mcp_servers = {}
    
    # BrightData
    bd_key = api_keys.get("BRIGHT_DATA_API_KEY") if api_keys else os.environ.get("BRIGHT_DATA_API_KEY")
    if bd_key:
        mcp_servers["bright_data"] = {
            "url": f"https://mcp.brightdata.com/mcp?token={bd_key}&pro=1",
            "transport": "streamable_http", # Reverted to streamable_http as per demo code
        }
    
    # Paper Search
    ps_key = api_keys.get("PAPER_SEARCH_API_KEY") if api_keys else os.environ.get("PAPER_SEARCH_API_KEY")
    if ps_key:
        mcp_servers["paper_search"] = {
            "url": f"https://server.smithery.ai/@adamamer20/paper-search-mcp-openai/mcp?api_key={ps_key}",
            "transport": "streamable_http",
        }

    custom_tools = [
        generate_search_queries, 
        execute_searches_and_get_urls,
        ingest_knowledge, 
        query_knowledge,
        format_paper_analysis,
        format_linkedin_profile
    ]

    # Try to connect to MCP servers if configured
    if mcp_servers:
        try:
            # Using async context manager usually, but MultiServerMCPClient might need specific handling
            # For simplicity in this demo, we assume direct initialization if supported or handle clean up
            # Note: langchain-mcp-adapters usage pattern:
            _mcp_client = MultiServerMCPClient(mcp_servers)
            # await _mcp_client.__aenter__() # Manually enter context if needed, or use context manager wrapper
            # For now let's try to get tools directly. If it fails, we fallback.
            try:
                _mcp_tools = await _mcp_client.get_tools()
                print(f"âœ… [Agent Service] Loaded {len(_mcp_tools)} MCP tools.")
            except Exception as e:
                print(f"âš ï¸ [Agent Service] Failed to load MCP tools: {e}")
                _mcp_tools = []
        except Exception as e:
            print(f"âš ï¸ [Agent Service] Failed to connect to MCP servers: {e}")
            _mcp_tools = []
    else:
        print("â„¹ï¸ [Agent Service] No MCP keys provided, skipping MCP initialization.")
        _mcp_tools = []

    all_tools = _mcp_tools + custom_tools

    # 2. Configure LLM
    # Use Gemini 2.5 Flash or Pro as the brain
    if "GOOGLE_API_KEY" not in os.environ:
        raise ValueError("GOOGLE_API_KEY is missing!")
        
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash", # æ ¹æ®ç”¨æˆ·æŒ‡ç¤ºä½¿ç”¨ gemini-2.5-flash
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0
    )

    # 3. Create LangGraph Agent
    checkpointer = InMemorySaver()
    
    # ä¸ºäº†æœ€å¤§ç¨‹åº¦å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ LangGraphï¼Œæˆ‘ä»¬ä¸å†å‘ create_react_agent ä¼ é€’ system prompt
    # è€Œæ˜¯é€‰æ‹©åœ¨è°ƒç”¨ agent æ—¶æ‰‹åŠ¨æ·»åŠ  SystemMessageã€‚
    # è¿™æ ·å¯ä»¥é¿å… state_modifier / messages_modifier å‚æ•°åä¸åŒ¹é…çš„é—®é¢˜ã€‚
    _agent_executor = create_react_agent(
        model=llm,
        tools=all_tools,
        checkpointer=checkpointer
    )
    
    print("âœ… [Agent Service] Agent initialized successfully.")
    return _agent_executor

async def get_agent_executor():
    """Get the singleton agent executor, initializing it if necessary."""
    global _agent_executor
    if _agent_executor is None:
        await initialize_agent()
    return _agent_executor

async def chat_with_agent(message: str, thread_id: str, api_keys: Dict[str, str] = None):
    """
    Main entry point for chatting with the agent.
    """
    # Re-initialize if new keys are provided (simple logic for now)
    if api_keys:
        await initialize_agent(api_keys)
    
    agent = await get_agent_executor()
    
    config = {"configurable": {"thread_id": thread_id}}
    
    # Invoke the graph
    # æ‰‹åŠ¨æ³¨å…¥ System Prompt ä»¥ç¡®ä¿å…¼å®¹æ€§
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=message)
    ]
    
    final_state = await agent.ainvoke(
        {"messages": messages},
        config=config
    )
    
    # Extract the last message
    last_message = final_state["messages"][-1]
    return last_message.content

async def cleanup_mcp():
    """Cleanup MCP client resources."""
    global _mcp_client
    if _mcp_client:
        # await _mcp_client.__aexit__(None, None, None) # If using context manager manual control
        pass
