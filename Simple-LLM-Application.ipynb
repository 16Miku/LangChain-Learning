{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e64c5d-06e9-4637-b9f6-a1e348f76061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官方文档教程\n",
    "# https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7796fde-58be-4d5e-99fc-c26a1e92ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用聊天模型和提示模板构建简单的 LLM 应用程序\n",
    "# 在本快速入门中，我们将向您展示如何使用 LangChain 构建一个简单的 LLM 应用程序。该应用程序会将文本从英语翻译成另一种语言。这是一个相对简单的 LLM 应用程序——只需一个 LLM 调用加上一些提示即可。尽管如此，这仍然是开始使用 LangChain 的好方法——只需一些提示和一个 LLM 调用就可以构建许多功能！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3f48d0e-e45b-411e-8b49-7b4e2c9300be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 LangChain\n",
    "# conda install langchain -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d693b2ac-974a-44d1-9a5b-431cf0528ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 LangChain 构建的许多应用程序都包含多个步骤，需要多次调用 LLM 函数。随着这些应用程序变得越来越复杂，能够检查链或代理内部究竟发生了什么变得至关重要。最好的方法是使用 LangSmith 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0585ab6c-6ab0-45a3-8c78-3d858d37faa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     # load environment variables from .env file (requires `python-dotenv`)\n",
    "#     from dotenv import load_dotenv\n",
    "\n",
    "#     load_dotenv()\n",
    "# except ImportError:\n",
    "#     pass\n",
    "\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "#         prompt=\"Enter your LangSmith API key (optional): \"\n",
    "#     )\n",
    "# if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "#     os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "#         prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "#     )\n",
    "#     if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "#         os.environ[\"LANGSMITH_PROJECT\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b30751b-30cb-42fe-a755-2a461e105cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用语言模型\n",
    "# pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0747ce7d-4906-4996-a48b-14964768a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先让我们直接使用模型。ChatModel 是 LangChain Runnables 的实例，这意味着它们暴露了一个用于交互的标准接口。为了简单地调用模型，我们可以将消息列表传递给 .invoke 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c757fd1-3a45-4896-8679-c9729ac5a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "286b30dd-79b7-48dd-bda0-5183bfcf5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请注意，ChatModel 接收消息对象作为输入，并生成消息对象作为输出。除了文本内容外，消息对象还传达对话角色并保存重要数据，例如工具调用次数和令牌使用次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "704cd7af-c9d2-4f0f-8462-75126574ae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--8bae7c8b-6291-4e58-b33d-5ed79ef33808-0', usage_metadata={'input_tokens': 9, 'output_tokens': 3, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c60e57e-4451-4c74-8d8a-1859d3041f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='嗨！ (Hāi!)\\n\\n你好！ (Nǐ hǎo!) - This is the more formal and common greeting.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d117f50b-d9ed-4a9c-91db-209960beb52e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 28, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Chinese\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c39d91f4-c5a8-48ce-8418-1177d7ae2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain 还支持通过字符串或 OpenAI 格式输入聊天模型。以下是等效的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df4d8d1b-375b-47c5-b056-f15d0e5cce88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--d7755c59-0466-4ad9-91ac-07cc824d207a-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77658ee6-c086-4efe-8c49-86211bb907a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--9a0ab176-c3fa-48cf-9117-82e0a0dede9b-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "274b9db7-d646-4e12-af9c-ee0a00feff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--484ddb06-cdf2-4b3d-8c3b-0fd0f411a680-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(\"Hello\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047e5f2-7ef1-4f8a-b4d3-5ca8ea640303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7e3e009-b37e-46b8-a2cb-fbb5f0188b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流式调用Streaming\n",
    "# 由于聊天模型是 Runnable 对象 ，它们对外暴露了一个标准接口，其中包含异步和流式调用模式。这使我们能够从聊天模型中流式传输单个令牌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b64a6c37-cfc1-4eb6-a5f0-778655147f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嗨|！ (Hāi!) \n",
      "\n",
      "你好！ (Nǐ hǎo!)| - This is a more formal and common greeting.\n",
      "\n",
      "Both are perfectly acceptable ways| to say \"Hi!\" in Chinese. The first is a direct transliteration, the second is the standard greeting.\n",
      "|"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfde3a3-60c9-4c10-a015-62149887ba03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5035bc1d-bc12-4fc2-98d6-4c0103262799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  提示模板\n",
    "# 现在，我们将一个消息列表直接传递给语言模型。这个消息列表从何而来？通常，它由用户输入和应用程序逻辑组合而成。应用程序逻辑通常接收原始用户输入，并将其转换为可传递给语言模型的消息列表。常见的转换包括添加系统消息或使用用户输入格式化模板。\n",
    "# 提示模板是 LangChain 中一个旨在协助这种转换的概念。它们接收原始用户输入并返回可传递给语言模型的数据（提示）。\n",
    "# 让我们在这里创建一个提示模板。它将接受两个用户变量：\n",
    "# language ：将文本翻译成的语言\n",
    "# text ：要翻译的文本\n",
    "# 请注意， ChatPromptTemplate 在单个模板中支持多种消息角色 。我们将 language 参数格式化为系统消息，将用户 text 格式化为用户消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bac05953-eb46-461a-b621-0b9eb4ee75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "466cf2b1-f10f-41ba-a3ec-41bd9abaa906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个提示模板的输入是一个字典。我们可以单独试用一下这个提示模板，看看它能做什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75c6713b-6aee-4b30-8214-30715eae40fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Chinese', additional_kwargs={}, response_metadata={}), HumanMessage(content='Saito Asuka', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Chinese\", \"text\": \"Saito Asuka\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edca9072-4383-4907-bac7-4120c6fe4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以看到它返回了一个包含两条消息的 ChatPromptValue 。如果我们想直接访问这些消息，我们可以这样做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9089b86-1382-4c33-b1c1-3b7c45473631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Chinese', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Saito Asuka', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bbe8468-3852-406d-b4a3-0a1f1753a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后，我们可以在格式化的提示上调用聊天模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6aa8682-ebda-4caf-bafc-19536a2e0dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "斋藤飞鸟 (Zhāi téng Fēiniǎo)\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1aecfb-5a7d-490f-aff1-c4a996df04e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ee322c4-6c1b-4cb8-8325-995796892c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结论\n",
    "# 在本教程中，您学习了如何创建第一个简单的 LLM 应用程序。您学习了如何使用语言模型、如何创建提示模板，以及如何在您使用 LangSmith 创建的应用程序中获得出色的可观察性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf788c-b368-41b9-a121-132601bc3ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb9c54-ac1d-4290-bfb6-cf3edd27caf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f9beb-7d3b-47bf-9f30-e45b5d106d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
